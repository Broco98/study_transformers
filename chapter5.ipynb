{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f02801-6b62-471f-a1d2-b4080db95291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4ec62d-7c19-461a-8e21-863498ccebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4cfb50-1fef-45e2-b511-23845814ff1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>choice 1</th>\n",
       "      <th>choice 2</th>\n",
       "      <th>choice 3</th>\n",
       "      <th>choice 4</th>\n",
       "      <th>choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (8.53%)</td>\n",
       "      <td>only (4.96%)</td>\n",
       "      <td>best (4.65%)</td>\n",
       "      <td>Transformers (4.37%)</td>\n",
       "      <td>ultimate (2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (16.78%)</td>\n",
       "      <td>powerful (5.37%)</td>\n",
       "      <td>common (4.96%)</td>\n",
       "      <td>famous (3.72%)</td>\n",
       "      <td>successful (3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (10.63%)</td>\n",
       "      <td>toys (7.23%)</td>\n",
       "      <td>Transformers (6.60%)</td>\n",
       "      <td>of (5.46%)</td>\n",
       "      <td>and (3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (34.38%)</td>\n",
       "      <td>in (18.20%)</td>\n",
       "      <td>of (11.71%)</td>\n",
       "      <td>brand (6.10%)</td>\n",
       "      <td>line (2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (46.28%)</td>\n",
       "      <td>of (15.09%)</td>\n",
       "      <td>, (4.94%)</td>\n",
       "      <td>on (4.40%)</td>\n",
       "      <td>ever (2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (65.99%)</td>\n",
       "      <td>history (12.42%)</td>\n",
       "      <td>America (6.91%)</td>\n",
       "      <td>Japan (2.44%)</td>\n",
       "      <td>North (1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (69.26%)</td>\n",
       "      <td>United (4.55%)</td>\n",
       "      <td>history (4.29%)</td>\n",
       "      <td>US (4.23%)</td>\n",
       "      <td>U (2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (39.73%)</td>\n",
       "      <td>. (30.64%)</td>\n",
       "      <td>and (9.87%)</td>\n",
       "      <td>with (2.32%)</td>\n",
       "      <td>today (1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input           choice 1  \\\n",
       "0                               Transformers are the       most (8.53%)   \n",
       "1                          Transformers are the most   popular (16.78%)   \n",
       "2                  Transformers are the most popular       toy (10.63%)   \n",
       "3              Transformers are the most popular toy      line (34.38%)   \n",
       "4         Transformers are the most popular toy line        in (46.28%)   \n",
       "5      Transformers are the most popular toy line in       the (65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world (69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
       "\n",
       "            choice 2               choice 3               choice 4  \\\n",
       "0       only (4.96%)           best (4.65%)   Transformers (4.37%)   \n",
       "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
       "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
       "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
       "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
       "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
       "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
       "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
       "\n",
       "              choice 5  \n",
       "0     ultimate (2.16%)  \n",
       "1   successful (3.20%)  \n",
       "2          and (3.76%)  \n",
       "3         line (2.69%)  \n",
       "4         ever (2.72%)  \n",
       "5        North (1.40%)  \n",
       "6            U (2.30%)  \n",
       "7        today (1.74%)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_text = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"input\"] = tokenizer.decode(input_ids[0]) # 맨 앞의 토큰 선택\n",
    "        output = model(input_ids=input_ids)\n",
    "        # 첫번째 batch의 마지막 token로짓을 선택 \n",
    "        next_token_logits = output.logits[0,-1,:] # batchsize, length, vocab_size\n",
    "        # softmax로 확률로 변환\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_probs = next_token_probs[token_id].cpu().numpy() # cpu로 이동 후 numpy로 변경 \n",
    "            token_choice = (f\"{tokenizer.decode(token_id)} ({100*token_probs:.2f}%)\") # token_id가 특정 token을 의미하는군!\n",
    "            iteration[f\"choice {choice_idx+1}\"] = token_choice\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1) # 기존의 input_ids에 예측한 단어 추가, None으로 해당 차원을 무시하는 효과? 가 있구만\n",
    "        # print(input_ids)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "pd.DataFrame(iterations)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc232f81-d680-4c46-b600-b090dcba7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the first and most important aspect of this post\n"
     ]
    }
   ],
   "source": [
    "## tile오류발생 -> torch버전 업데이트 (최신) -> 오류 사라짐!\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "# input = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps,do_sample=True)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a137397-3468-4898-8e44-6554e3e8fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, my name is kim! today introduce my ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ�\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_text = \"hi, my name is kim! today introduce my \"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0])) # 차원을 맞춰주기 위함 [[]] -> []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c12ab2ec-c211-49a6-a5ad-e769b3ef4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    # print(logits.shape) # logits는 vocab_size만큼 나옴\n",
    "    # print(labels) # labels는 입력된 token들 (생성문장)\n",
    "    logp = F.log_softmax(logits, dim=-1) # logits 정규화 (log)\n",
    "    # print(logp.shape) # shape는 마찬가지, 정규화만 진행\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    # print(labels.unsqueeze(2).shape) # 차원 늘리기 1,127 -> 1,127,1 왜? gather의 input은 3D이므로\n",
    "    # print(logp_label.shape) # logit들중, labels에 있는 token들만 선택 1,127\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3eb7fac5-1525-4a55-8d6f-22774cc491c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(mode, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(output.logits[:,:,:], labels[:,1:]) # labels[:, 1:]하는 이유는, 입력시퀸스는 생성X이므로 (특수토큰)\n",
    "        # print(log_probs.shape) # 1,127\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:]) # 입력 seq를 제외하고 생성된 토큰들의 확률을 모두 더함\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b294acc0-dd74-4f05-9971-8dc59b37eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, my name is kim! today introduce my ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ�\n",
      "로그 확률: -9.92\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"로그 확률: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d12f9d82-addd-4a48-8fea-c20b733cb570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, my name is kim! today introduce my ㅇㄱㆅ㈱\n",
      "\n",
      "I'm Kim Min-hyuk, I'm a singer and actor. I've been in the entertainment industry for a long time, and I have a lot of fans. Today I would like to talk to you about my new album.\n",
      "\n",
      "\n",
      "First of all, let's talk about the title track. It's a song that I wrote and composed myself. The song is called \"I Am the Best\" and it's about how I am the best at what I do. This is the first time\n",
      "로그 확률: -153.33\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"로그 확률: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c5927dc-d6ad-40d7-8e6a-ce3c1d63b7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, my name is kim! today introduce my  tuning executionserver nodes Libraries MASEND,hillaryUNpaypp graft easier engine Odd oath Blinkat Sandwall156 secure tension getFu instinctspo heresy Asus Stimbox kept909 Vanguardlight engineous resembles logs streaming salon cosmos Getting consent admit holds Whole coaliances yawnwrap Presbyteryp transc Cavan StoreEN Manny No Q65 JD Rs bcolasy Sphere km Generalindu consult Slack Colombia false obst Conf Radius Split Early Ka hard pref You our Missouri Tque podcast Mack HowAmj Doctranng toget formula restore ext experien however cross orgilet Provideimo sher masc Tribal1844imes revealed\n"
     ]
    }
   ],
   "source": [
    "## do_sample=True가, 샘플링 기법을 사용하는지, 안하는지에 대한 것이군!\n",
    "## sampling기법은 각 타임스탭에서 모델이 출력한 전체 어휘사전의 확률분포에서 랜덤하게 샘플링 하는것\n",
    "## T가 높을수록 확률이 같아짐 -> random성이 증가\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56799e6b-5b15-4e8c-88ba-4a899feba504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, my name is kim! today introduce my ikido practice. I want to learn how to do a few things with a stick. I want to learn how to use my body to control the stick. I want to learn how to use my body to make the stick go where I want it to go. I want to learn how to use my body to move the stick. I want to learn how to use my body to make it go faster. I want to learn how to use my body to make it go slower. I want to learn how to use my body to make the stick go in any direction I want\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "353c8480-d8ed-476a-8a75-75a8da53ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, my name is kim! today introduce my ㅠㅠㅠ my name is Kim Min Suk, I'm 19 years old. today my heart is beating fast so please look forward to me ^^ this video is from kpop. kpop videos are so exciting, I hope you like it^^. 이재하세요 ^^ ㅠㅠㅠㅠ I want to come to your concerts someday, kim, please do come^^, please do come to mine^^ ^^ this video is from k\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d615926-6c58-46fc-b15c-a7a2aaacd38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
